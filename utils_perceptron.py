# -*- coding: utf-8 -*-
"""v2_utils_perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lXl7zoaWTalP0kLqnyHRKVsadV-7gs1f

#Importing libraries
"""

########### data manipulation #########
import pandas as pd
import numpy as np

########## graphics ########
import matplotlib.pyplot as plt
import seaborn as sns

####### stats #########
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

###### misc #######
import sys
import pickle
import itertools
from tqdm import tqdm

RESET = '\033[0m'  # Resets the color to default.
RED = '\033[31m'    # Red text
GREEN = '\033[32m'  # Green tex
BLUE = '\033[34m'   # Blue text

"""#Data processing"""

def df_prepare(df):
  df.columns = [
    "ID number", "Diagnosis",
    "Radius mean", "Radius se", "Radius worst",
    "Texture mean", "Texture se", "Texture worst",
    "Perimeter mean", "Perimeter se", "Perimeter worst",
    "Area mean", "Area se", "Area worst",
    "Smoothness mean", "Smoothness se", "Smoothness worst",
    "Compactness mean", "Compactness se", "Compactness worst",
    "Concavity mean", "Concavity se", "Concavity worst",
    "Concave points mean", "Concave points se", "Concave points worst",
    "Symmetry mean", "Symmetry se", "Symmetry worst",
    "Fractal dimension mean", "Fractal dimension se", "Fractal dimension worst"]

  df['Diagnosis'] = df['Diagnosis'].replace({'B': 0, 'M': 1})
  df['Diagnosis'] = df['Diagnosis'].astype(int)

  df.set_index('ID number', inplace=True)

  return df

def df_prepare_data(df):
  df.columns = [
    "ID number", "Diagnosis",
    "Radius mean", "Radius se", "Radius worst",
    "Texture mean", "Texture se", "Texture worst",
    "Perimeter mean", "Perimeter se", "Perimeter worst",
    "Area mean", "Area se", "Area worst",
    "Smoothness mean", "Smoothness se", "Smoothness worst",
    "Compactness mean", "Compactness se", "Compactness worst",
    "Concavity mean", "Concavity se", "Concavity worst",
    "Concave points mean", "Concave points se", "Concave points worst",
    "Symmetry mean", "Symmetry se", "Symmetry worst",
    "Fractal dimension mean", "Fractal dimension se", "Fractal dimension worst"]

  df['Diagnosis'] = df['Diagnosis'].astype('category')

  df.set_index('ID number', inplace=True)

  return df

def get_valid_fraction():
    while True:
        try:
            # Prompt the user with colored text using concatenation
            fraction = float(input(GREEN + "Enter a fraction between 0.25 and 0.95 of the samples to train the model (usually 0.80): " + RESET))

            # Check if the fraction is within the valid range
            if 0.25 <= fraction <= 0.95:
                return fraction
            else:
                print(RED + "Error: Fraction must be between 0.25 and 0.95." + RESET)
        except ValueError:
            print(RED + "Error: Please enter a valid float number." + RESET)

def train_test(df):
    frac = get_valid_fraction()
    df_train = df.sample(frac=frac, random_state=42)
    df_test = df.drop(df_train.index)

    print(f"\n{BLUE}Creating df_train with {frac*100:.0f}% of the rows:{RESET}")
    print(f"\n df_train.shape: {df_train.shape}")
    print(f"\n{BLUE}Creating df_test with {(1-frac)*100:.0f}% of the rows:{RESET}")
    print(f"\ndf_test.shape: {df_test.shape}")

    return df_train, df_test

def convert_to_one_hot(Y):
    """
    Convert a binary label numpy array into a one-hot encoded format.

    Args:
    Y (numpy.ndarray): Binary label array of shape (1, n_samples).

    Returns:
    numpy.ndarray: One-hot encoded array of shape (2, n_samples).
    """
    # Create an array of zeros with shape (2, number of samples)
    one_hot = np.zeros((2, Y.shape[1]))
    # Set the appropriate indices: for label 1 at position (1, :) and label 0 at position (0, :)
    one_hot[1, :] = Y  # Set second row to Y, where Y==1 this row gets 1
    one_hot[0, :] = 1 - Y  # Set first row to 1-Y, where Y==0 this row gets 1
    return one_hot

def train_test_data(df_train, df_test):

  #trainig data
  y_train = df_train['Diagnosis'].values.reshape(1, -1)  # Reshape for compatibility with matrix operations
  #y_train.shape
  y_train = convert_to_one_hot(y_train)
  X_train_raw = df_train.drop('Diagnosis', axis=1).values# numpy.npdarray (1000,3)
  #scaler = StandardScaler()
  scaler = MinMaxScaler()
  X_train = scaler.fit_transform(X_train_raw).T  # Transpose for matching neural network dimensions

  #testing data
  y_test = df_test['Diagnosis'].values.reshape(1, -1)  # Reshape for compatibility with matrix operations
  #y_test.shape
  y_test = convert_to_one_hot(y_test)
  #y_test.shape
  X_test_raw = df_test.drop('Diagnosis', axis=1).values# numpy.npdarray (1000,3)
  #X_test_raw.shape
  X_test = scaler.transform(X_test_raw).T  # Transpose for matching neural network dimensions

  return X_train, X_test, y_train, y_test

"""#Plotting histograms - scatter plots"""

def plot_categorical_distribution(df, variable_name):
    """
    histogram with distinct colors for each category
    displays a table below with class names, their frequency,
    and relative frequency.

    :param df: DataFrame
    :param variable_name: The name of the categorical column to be analyzed.
    """
    # Set the aesthetic style of the plots
    sns.set(style="whitegrid")

    # Create the plot figure
    plt.figure(figsize=(8, 6))
    ax = plt.subplot(211)  # Histogram on the top half of the figure
    #2(rows)1(column)1(first plot)

    # Calculate counts
    counts = df[variable_name].value_counts()
    categories = counts.index

    # Get unique colors for each category using seaborn palette
    colors = sns.color_palette("Set2", len(categories))

    # Create a bar plot using matplotlib
    bars = ax.bar(categories, counts, color=colors)
    ax.set_title(f'Distribution of {variable_name}')
    ax.set_xlabel(variable_name)
    ax.set_ylabel('Count')

    # Calculate the frequency and relative frequency
    total = df[variable_name].notnull().sum()  # total non-null entries
    freq_df = pd.DataFrame({
        variable_name: categories,
        'Frequency': counts,
        'Relative Frequency': (counts / total * 100).round(1)
    })

    # Plot table on the bottom half of the figure
    ax_table = plt.subplot(212)  # Table in the bottom half
    ax_table.axis('off')  # Turn off the axes for the table subplot
    table = plt.table(cellText=freq_df.values, colLabels=freq_df.columns, cellLoc='center', loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.2)  # Scale the table to fit the subplot

    # Layout adjustment
    plt.tight_layout()
    plt.show()

def plot_histograms(df):
    """
    Plots the distribution of all numerical variables in the DataFrame `df` with respect to the
    'Diagnosis' categories, showing a 4-column grid for each row of two variables.

    :param df: DataFrame containing numerical variables and 'Diagnosis' columns.
    """
    # Set the aesthetic style of the plots
    sns.set(style="whitegrid")

    # Identify all numerical columns in the DataFrame
    numerical_columns = df.select_dtypes(include=['number']).columns

    # Calculate the number of rows needed
    num_vars = len(numerical_columns)
    num_rows = (num_vars + 1) // 2  # Use ceiling division to ensure enough rows

    for i in range(0, num_vars, 2):
        # Create a figure with a 1 row and 4 columns grid of subplots
        fig, axes = plt.subplots(1, 4, figsize=(24, 5))  # Double the width to accommodate 4 plots per row
        plt.suptitle(f'Distribution of Variables by Category and Total', fontsize=16)

        # First variable in the row
        if i < num_vars:
            sns.histplot(df[numerical_columns[i]], kde=True, color="skyblue", ax=axes[0])
            axes[0].set_title(f'Histogram and KDE of {numerical_columns[i]}', color = 'magenta')
            axes[0].set_xlabel(numerical_columns[i])
            axes[0].set_ylabel('Frequency')

            sns.histplot(data=df, x=numerical_columns[i], hue='Diagnosis', kde=True, element="step",
                         palette={'B': 'green', 'M': 'red'}, ax=axes[1])
            axes[1].set_title(f'Histogram and KDE of {numerical_columns[i]} by Diagnosis', color = 'magenta')
            axes[1].set_xlabel(numerical_columns[i])
            axes[1].set_ylabel('Frequency')

        # Second variable in the row
        if i + 1 < num_vars:
            sns.histplot(df[numerical_columns[i+1]], kde=True, color="skyblue", ax=axes[2])
            axes[2].set_title(f'Histogram and KDE of {numerical_columns[i+1]}', color = 'navy')
            axes[2].set_xlabel(numerical_columns[i+1])
            axes[2].set_ylabel('Frequency')

            sns.histplot(data=df, x=numerical_columns[i+1], hue='Diagnosis', kde=True, element="step",
                         palette={'B': 'green', 'M': 'red'}, ax=axes[3])
            axes[3].set_title(f'Histogram and KDE of {numerical_columns[i+1]} by Diagnosis', color = 'navy')
            axes[3].set_xlabel(numerical_columns[i+1])
            axes[3].set_ylabel('Frequency')

        # Adjust layout for current row of plots
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.show()

def plot_highly_correlated_pairs(df, threshold=0.7):
    """
    Scatter plots for pairs with a correlation
    greater than threshold, displayed in a 4-column grid,
    avoiding symmetrical plots. Points are colored based on 'Diagnosis'.

    :param df: DataFrame.
    :param threshold: considering high correlations (default is 0.7).
    """
    # Select only the numerical columns from the DataFrame
    numerical_df = df.select_dtypes(include=['number'])

    # Ensure 'Diagnosis' is included for coloring
    df = df[numerical_df.columns.tolist() + ['Diagnosis']]

    # Calculate the correlation matrix for the numerical columns
    correlation_matrix = numerical_df.corr()

    # Create a mask to identify pairs to plot once
    tri_upper = pd.DataFrame(np.triu(np.ones(correlation_matrix.shape), k=1), columns=correlation_matrix.columns, index=correlation_matrix.columns)

    # Find pairs with correlation greater than threshold or less than -threshold
    strong_pairs = correlation_matrix.where(tri_upper > 0).stack().reset_index()
    strong_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']
    strong_pairs = strong_pairs.loc[(strong_pairs['Correlation'] > threshold) |
                                    (strong_pairs['Correlation'] < -threshold)]

    # Sort strong_pairs by absolute correlation value
    strong_pairs = strong_pairs.iloc[strong_pairs['Correlation'].abs().argsort()[::-1]].reset_index(drop=True)

    num_plots = len(strong_pairs)
    print(BLUE + f"Number of plots to be generated: {num_plots}\n" + RESET)

    # Determine the number of rows needed for the subplot grid
    num_plots = len(strong_pairs)
    num_columns = 4
    num_rows = (num_plots + num_columns - 1) // num_columns  # Ceiling division

    fig, axes = plt.subplots(num_rows, num_columns, figsize=(20, 4 * num_rows))  # Adjust figure size as necessary
    #fig.suptitle('Highly Correlated Variable Pairs', fontsize=16, color = 'navy')

    # Plot each pair in the grid
    for index, (i, row) in enumerate(strong_pairs.iterrows()):
        ax = axes[index // num_columns, index % num_columns]
        sns.scatterplot(data=df, x=row['Variable 1'], y=row['Variable 2'], hue='Diagnosis',
                        palette={'B': 'green', 'M': 'red'}, ax=ax)
        ax.set_title(f"{row['Variable 1']} vs {row['Variable 2']}\nCorr: {row['Correlation']:.2f}")
        ax.set_xlabel(row['Variable 1'])
        ax.set_ylabel(row['Variable 2'])

    # Adjust layout and remove empty subplots if any
    for ax in axes.flatten()[index+1:]:
        ax.axis('off')  # Hide unused subplots

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

def plot_low_correlated_pairs(df, threshold=0.2):
    """
    Scatter plots for pairs with a correlation
    lower than threshold, displayed in a 4-column grid,
    avoiding symmetrical plots. Points are colored based on 'Diagnosis'.

    :param df: DataFrame.
    :param threshold: considering low correlations (default is 0.2).
    """
    # Select only the numerical columns from the DataFrame
    numerical_df = df.select_dtypes(include=['number'])

    # Ensure 'Diagnosis' is included for coloring
    df = df[numerical_df.columns.tolist() + ['Diagnosis']]

    # Calculate the correlation matrix for the numerical columns
    correlation_matrix = numerical_df.corr()

    # Create a mask to identify pairs to plot once
    tri_upper = pd.DataFrame(np.triu(np.ones(correlation_matrix.shape), k=1), columns=correlation_matrix.columns, index=correlation_matrix.columns)

    # Find pairs with correlation greater than threshold or less than -threshold
    weak_pairs = correlation_matrix.where(tri_upper > 0).stack().reset_index()
    weak_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']
    weak_pairs = weak_pairs.loc[(weak_pairs['Correlation'] > - threshold) &
                                    (weak_pairs['Correlation'] < threshold)]

    # Sort strong_pairs by absolute correlation value
    weak_pairs = weak_pairs.iloc[weak_pairs['Correlation'].abs().argsort()[::-1]].reset_index(drop=True)

    num_plots = len(weak_pairs)
    print(BLUE + f"Number of plots to be generated: {num_plots}\n" + RESET)

    # Determine the number of rows needed for the subplot grid
    num_plots = len(weak_pairs)
    num_columns = 4
    num_rows = (num_plots + num_columns - 1) // num_columns  # Ceiling division

    fig, axes = plt.subplots(num_rows, num_columns, figsize=(20, 4 * num_rows))  # Adjust figure size as necessary
    #fig.suptitle('Highly Correlated Variable Pairs', fontsize=16, color = 'navy')

    # Plot each pair in the grid
    for index, (i, row) in enumerate(weak_pairs.iterrows()):
        ax = axes[index // num_columns, index % num_columns]
        sns.scatterplot(data=df, x=row['Variable 1'], y=row['Variable 2'], hue='Diagnosis',
                        palette={'B': 'green', 'M': 'red'}, ax=ax)
        ax.set_title(f"{row['Variable 1']} vs {row['Variable 2']}\nCorr: {row['Correlation']:6f}")
        ax.set_xlabel(row['Variable 1'])
        ax.set_ylabel(row['Variable 2'])

    # Adjust layout and remove empty subplots if any
    for ax in axes.flatten()[index+1:]:
        ax.axis('off')  # Hide unused subplots

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

"""#Correlation analysis - VIF"""

def get_high_correlations(df, threshold=0.8):
    """
    Calculate and retrieve correlations that are above a specified threshold in absolute value,
    excluding the 'Diagnosis' and 'ID number' columns, sorted by the absolute value of the correlation,
    and excluding symmetric duplicates. Also prints the count of displayed correlations.

    :param df: The input DataFrame.
    :param threshold: The threshold for filtering correlations (default is 0.8).
    :return: DataFrame with the high correlations.
    """
    # Drop the columns 'Diagnosis' and 'ID number', ignoring errors if they are not present
    df = df.drop(columns=['Diagnosis', 'ID number'], errors='ignore')

    # Calculate the correlation matrix of the numerical columns
    correlation_matrix = df.corr()

    # Flatten the correlation matrix to a DataFrame and reset index to convert to column
    high_correlations = correlation_matrix.stack().reset_index()
    high_correlations.columns = ['Variable 1', 'Variable 2', 'Correlation']

    # Filter to keep only one instance of each correlation pair, avoiding duplicates
    high_correlations = high_correlations[high_correlations['Variable 1'] < high_correlations['Variable 2']]

    # Filter to include only correlations where the absolute value is above the threshold
    # and the correlation is not 1 (to exclude self-correlation of variables)
    high_correlations = high_correlations[
        (high_correlations['Correlation'].abs() > threshold) & (high_correlations['Correlation'] != 1)
    ]

    # Sort by the absolute value of correlations
    high_correlations['Abs_Correlation'] = high_correlations['Correlation'].abs()
    high_correlations = high_correlations.sort_values(by='Abs_Correlation', ascending=False).drop('Abs_Correlation', axis=1)

    # Print the number of correlations that meet the criteria
    print(BLUE + f"Number of significant correlations (|Correlation| > {threshold}): {len(high_correlations)}\n" + RESET)

    return high_correlations

def get_low_correlations(df, threshold=0.10):
    """
    Calculate and retrieve correlations that are bellow a specified threshold in absolute value,
    excluding the 'Diagnosis' and 'ID number' columns, sorted by the absolute value of the correlation,
    and excluding symmetric duplicates. Also prints the count of displayed correlations.

    :param df: The input DataFrame.
    :param threshold: The threshold for filtering correlations (default is 0.10).
    :return: DataFrame with the high correlations.
    """
    # Drop the columns 'Diagnosis' and 'ID number', ignoring errors if they are not present
    df = df.drop(columns=['Diagnosis', 'ID number'], errors='ignore')

    # Calculate the correlation matrix of the numerical columns
    correlation_matrix = df.corr()

    # Flatten the correlation matrix to a DataFrame and reset index to convert to column
    low_correlations = correlation_matrix.stack().reset_index()
    low_correlations.columns = ['Feature 1', 'Feature 2', 'Correlation']

    # Filter to keep only one instance of each correlation pair, avoiding duplicates
    low_correlations = low_correlations[low_correlations['Feature 1'] < low_correlations['Feature 2']]

    # Filter to include only correlations where the absolute value is bellow the threshold
    low_correlations = low_correlations[
        (low_correlations['Correlation'].abs() < threshold)
    ]

    # Sort by the absolute value of correlations
    low_correlations['Abs_Correlation'] = low_correlations['Correlation'].abs()
    low_correlations = low_correlations.sort_values(by='Abs_Correlation', ascending=True).drop('Abs_Correlation', axis=1)

    # Print the number of correlations that meet the criteria
    print(BLUE + f"Number of low correlations (|Correlation| < {threshold}): {len(low_correlations)}\n" + RESET)

    return low_correlations

def remove_high_vif_features(df, threshold):
    """
    Removes the feature with the highest VIF until all features have a VIF below
    the specified threshold. The 'Diagnosis' feature is removed initially.

    :param df: A pandas DataFrame containing the features.
    :param threshold: The VIF threshold for removing features.
    :return: A DataFrame with all features below the specified VIF threshold.
    """
    # Ensure 'Diagnosis' is removed first
    df = df.drop(columns=['Diagnosis', 'ID number'], errors='ignore')

    # Initialize an empty set for removed features
    removed_features = set()

    def calculate_vif(df):
        """
        Calculates the Variance Inflation Factor (VIF) for each numerical variable
        in the DataFrame, excluding any already removed features.

        :param df: A pandas DataFrame containing the features.
        :return: A DataFrame containing the VIF for each feature.
        """
        numerical_df = df.select_dtypes(include=[np.number])
        numerical_df = add_constant(numerical_df)

        vif_data = pd.DataFrame()
        vif_data["Feature"] = numerical_df.columns
        vif_data["VIF"] = [
            variance_inflation_factor(numerical_df.values, i)
            for i in range(numerical_df.shape[1])
        ]
        return vif_data

    while True:
        # Calculate the VIF
        vif_data = calculate_vif(df)

        # Ignore the constant term from VIF data
        vif_data = vif_data[vif_data['Feature'] != 'const']

        # Find the feature with the highest VIF
        highest_vif_feature = vif_data.loc[vif_data['VIF'].idxmax()]

        # If the highest VIF is below the threshold, break the loop
        if highest_vif_feature['VIF'] < threshold:
            break

        # Print feature name and its VIF value
        print(f"Removing {highest_vif_feature['Feature']} with VIF: {highest_vif_feature['VIF']:.2f}")

        # Remove the feature with the highest VIF from the DataFrame
        df = df.drop(columns=[highest_vif_feature['Feature']])
        removed_features.add(highest_vif_feature['Feature'])
    print(BLUE + f"number of remaining variables: {df.shape[1]}" + RESET)
    return df

"""#Preceptron training"""

def initialize_parameters(n_features, n_hidden_layers, n_neurons_per_layer):
    np.random.seed(42)
    output = 2  # Hardcoded output layer size

    parameters = {}
    layer_input_size = n_features

    # Initialize parameters for each hidden layer
    for i in range(1, n_hidden_layers + 1):
        layer_output_size = n_neurons_per_layer[i - 1]
        parameters[f'W{i}'] = np.random.randn(layer_output_size, layer_input_size) * 0.1
        parameters[f'b{i}'] = np.zeros((layer_output_size, 1))
        layer_input_size = layer_output_size

    # Initialize parameters for the output layer
    parameters[f'W{n_hidden_layers + 1}'] = np.random.randn(output, layer_input_size) * 0.1
    parameters[f'b{n_hidden_layers + 1}'] = np.zeros((output, 1))

    return parameters

def softmax(Z):
    e_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))
    return e_Z / e_Z.sum(axis=0, keepdims=True)

def relu(Z):
    rel_Z = np.maximum(0, Z)
    #print("rel_Z shape", rel_Z.shape)
    return rel_Z

def sigmoid(Z):
    sig_Z = 1 / (1 + np.exp(-Z))
    #print("sig_Z shape", sig_Z.shape)
    return sig_Z

def leaky_relu(Z, alpha=0.00001):
    return np.where(Z > 0, Z, alpha * Z)

def relu_derivative(Z):
    """Compute the derivative of the ReLU activation function."""
    return np.int64(Z > 0)

def sigmoid_derivative(Z):
    """Compute the derivative of the sigmoid activation function."""
    return sigmoid(Z) * (1 - sigmoid(Z))

def leaky_relu_derivative(Z, alpha=0.00001):
    """
    Derivative of the Leaky ReLU activation function.

    Args:
    Z (numpy.ndarray): The input for which to compute the derivative.
    alpha (float): The negative slope coefficient.

    Returns:
    numpy.ndarray: The derivative of the Leaky ReLU function.
    """
    dZ = np.ones_like(Z)
    dZ[Z < 0] = alpha
    return dZ

def forward_pass(X, parameters, n_hidden_layers):
    cache = {}
    A = X

    for i in range(1, n_hidden_layers + 1):
        Z = np.dot(parameters[f'W{i}'], A) + parameters[f'b{i}']
        A = leaky_relu(Z, 0.01)
        cache[f'Z{i}'] = Z
        cache[f'A{i}'] = A

    Z_output = np.dot(parameters[f'W{n_hidden_layers + 1}'], A) + parameters[f'b{n_hidden_layers + 1}']
    A_output = softmax(Z_output)
    cache[f'Z{n_hidden_layers + 1}'] = Z_output
    cache[f'A{n_hidden_layers + 1}'] = A_output

    return A_output, cache

def compute_loss(A_output, Y):
    m = Y.shape[1]

    # Clip the predicted probabilities to avoid log(0)
    epsilon = 0.0000000001
    A_output = np.clip(A_output, epsilon, 1 - epsilon)

    # Compute the loss for both classes
    log_probs = - (Y * np.log(A_output) + (1 - Y) * np.log(1 - A_output))
    loss = np.sum(log_probs) / m

    return loss

def L2_compute_loss(A_output, y, parameters, reg_lambda=0.01):
    m = y.shape[0]

    # Clip the predicted probabilities to avoid log(0)
    epsilon = 1e-10
    A_output = np.clip(A_output, epsilon, 1 - epsilon)

    # Compute cross-entropy loss
    cross_entropy_loss = -np.mean(y * np.log(A_output) + (1 - y) * np.log(1 - A_output))

    # Compute L2 regularization cost
    L2_regularization_cost = (reg_lambda / (2 * m)) * sum(
        [np.sum(np.square(parameters['W' + str(l)])) for l in range(1, len(parameters) // 2 + 1)]
    )

    # Total loss is cross-entropy loss plus L2 regularization cost
    return cross_entropy_loss + L2_regularization_cost

def backward_pass(X, Y, cache, parameters, n_hidden_layers):
    m = X.shape[1]  # Number of examples
    gradients = {}

    # Extract weights and activation values from forward pass
    A_output = cache[f'A{n_hidden_layers + 1}']
    dZ = A_output - Y

    for i in range(n_hidden_layers + 1, 0, -1):
        A_prev = X if i == 1 else cache[f'A{i - 1}']
        dW = np.dot(dZ, A_prev.T) / m
        db = np.sum(dZ, axis=1, keepdims=True) / m
        gradients[f'dW{i}'] = dW
        gradients[f'db{i}'] = db

        if i > 1:
            dA_prev = np.dot(parameters[f'W{i}'].T, dZ)
            dZ = dA_prev * leaky_relu_derivative(cache[f'Z{i - 1}'], alpha=0.01)

    return gradients

def update_parameters(parameters, gradients, learning_rate):
    n_layers = len(parameters) // 2  # Assuming parameters contain W and b for each layer

    for i in range(1, n_layers + 1):
        parameters[f'W{i}'] -= learning_rate * gradients[f'dW{i}']
        parameters[f'b{i}'] -= learning_rate * gradients[f'db{i}']

    return parameters

def accuracy(y_real, y_pred):
    y_real = np.argmax(y_real, axis=0)
    y_pred = np.argmax(y_pred, axis=0)
    return np.mean(y_real == y_pred)

def batchs_model(X_train, y_train, X_test, y_test, num_iterations=1000, learning_rate=0.01,
          epoch_info=50, n_features=3, n_hidden_layers=2, n_neurons_per_layer=None,
          batch_size = 32):
    if n_neurons_per_layer is None:
        n_neurons_per_layer = [10, 10]  # Default neurons if not provided

    parameters = initialize_parameters(n_features, n_hidden_layers, n_neurons_per_layer)
    training_loss_list = []
    test_loss_list = []
    training_accuracy_list = []
    test_accuracy_list = []
    print(X_train.shape)
    print(batch_size)

    for i in range(num_iterations):

        indices = np.random.choice(X_train.shape[1], batch_size, replace=False)
        X_sample = X_train[:, indices]
        y_sample = y_train[:, indices]

        # Training - Forward pass
        A_output_train, cache_train = forward_pass(X_sample, parameters, n_hidden_layers)
        # Training - Compute loss
        loss_train = compute_loss(A_output_train, y_sample)
        # L2 loss_train = compute_loss(A_output_train, y_train, parameters, 0.01)
        training_loss_list.append(loss_train)
        # Training - Compute accuracy
        y_pred_train = (A_output_train > 0.5).astype(int)
        training_accuracy = accuracy(y_sample, y_pred_train)
        training_accuracy_list.append(training_accuracy)

        # Test - Forward pass
        A_output_test, cache_test = forward_pass(X_test, parameters, n_hidden_layers)
        # Test - Compute loss
        loss_test = compute_loss(A_output_test, y_test)
        # L2 loss_test = compute_loss(A_output_test, y_test, parameters, 0.01)
        test_loss_list.append(loss_test)
        # Test - Compute accuracy
        y_pred_test = (A_output_test > 0.5).astype(int)
        test_accuracy = accuracy(y_test, y_pred_test)
        test_accuracy_list.append(test_accuracy)

        # Backward pass and update parameters using training data
        gradients = backward_pass(X_sample, y_sample, cache_train, parameters, n_hidden_layers)
        parameters = update_parameters(parameters, gradients, learning_rate)

        if i % epoch_info == 0:
            print(f"epoch {i} / {num_iterations}: train_loss = {loss_train:.6f}, val_loss = {loss_test:.6f}")
            print(f"epoch {i} / {num_iterations}: train_accuracy = {training_accuracy:.6f}, val_accuracy = {test_accuracy:.6f}\n")

    print(f"final train accuracy:  {training_accuracy:.6f}")
    print(f"final test accuracy:  {test_accuracy:.6f}")

    return parameters, training_loss_list, training_accuracy_list, test_loss_list, test_accuracy_list

def model(X_train, y_train, X_test, y_test, num_iterations=1000, learning_rate=0.01,
          epoch_info=50, n_features=3, n_hidden_layers=2, n_neurons_per_layer=None):
    if n_neurons_per_layer is None:
        n_neurons_per_layer = [10, 10]  # Default neurons if not provided

    parameters = initialize_parameters(n_features, n_hidden_layers, n_neurons_per_layer)
    training_loss_list = []
    test_loss_list = []
    training_accuracy_list = []
    test_accuracy_list = []

    for i in range(num_iterations):

        # Training - Forward pass
        A_output_train, cache_train = forward_pass(X_train, parameters, n_hidden_layers)
        # Training - Compute loss
        loss_train = compute_loss(A_output_train, y_train)
        # L2 loss_train = compute_loss(A_output_train, y_train, parameters, 0.01)
        training_loss_list.append(loss_train)
        # Training - Compute accuracy
        y_pred_train = (A_output_train > 0.5).astype(int)
        training_accuracy = accuracy(y_train, y_pred_train)
        training_accuracy_list.append(training_accuracy)

        # Test - Forward pass
        A_output_test, cache_test = forward_pass(X_test, parameters, n_hidden_layers)
        # Test - Compute loss
        loss_test = compute_loss(A_output_test, y_test)
        # L2 loss_test = compute_loss(A_output_test, y_test, parameters, 0.01)
        test_loss_list.append(loss_test)
        # Test - Compute accuracy
        y_pred_test = (A_output_test > 0.5).astype(int)
        test_accuracy = accuracy(y_test, y_pred_test)
        test_accuracy_list.append(test_accuracy)

        # Backward pass and update parameters using training data
        gradients = backward_pass(X_train, y_train, cache_train, parameters, n_hidden_layers)
        parameters = update_parameters(parameters, gradients, learning_rate)

        if i % epoch_info == 0:
            print(f"epoch {i} / {num_iterations}: train_loss = {loss_train:.6f}, val_loss = {loss_test:.6f}")
            print(f"epoch {i} / {num_iterations}: train_accuracy = {training_accuracy:.6f}, val_accuracy = {test_accuracy:.6f}\n")

    print(f"final train accuracy:  {training_accuracy:.6f}")
    print(f"final test accuracy:  {test_accuracy:.6f}")

    return parameters, training_loss_list, training_accuracy_list, test_loss_list, test_accuracy_list

def quiet_model(X_train, y_train, X_test, y_test, num_iterations=1000, learning_rate=0.01,
          epoch_info=50, n_features=3, n_hidden_layers=2, n_neurons_per_layer=None):
    if n_neurons_per_layer is None:
        n_neurons_per_layer = [10, 10]  # Default neurons if not provided

    parameters = initialize_parameters(n_features, n_hidden_layers, n_neurons_per_layer)
    training_loss_list = []
    test_loss_list = []
    training_accuracy_list = []
    test_accuracy_list = []

    for i in range(num_iterations):

        # Training - Forward pass
        A_output_train, cache_train = forward_pass(X_train, parameters, n_hidden_layers)
        # Training - Compute loss
        loss_train = compute_loss(A_output_train, y_train)
        # L2 loss_train = compute_loss(A_output_train, y_train, parameters, 0.01)
        training_loss_list.append(loss_train)
        # Training - Compute accuracy
        y_pred_train = (A_output_train > 0.5).astype(int)
        training_accuracy = accuracy(y_train, y_pred_train)
        training_accuracy_list.append(training_accuracy)

        # Test - Forward pass
        A_output_test, cache_test = forward_pass(X_test, parameters, n_hidden_layers)
        # Test - Compute loss
        loss_test = compute_loss(A_output_test, y_test)
        # L2 loss_test = compute_loss(A_output_test, y_test, parameters, 0.01)
        test_loss_list.append(loss_test)
        # Test - Compute accuracy
        y_pred_test = (A_output_test > 0.5).astype(int)
        test_accuracy = accuracy(y_test, y_pred_test)
        test_accuracy_list.append(test_accuracy)

        # Backward pass and update parameters using training data
        gradients = backward_pass(X_train, y_train, cache_train, parameters, n_hidden_layers)
        parameters = update_parameters(parameters, gradients, learning_rate)

    return parameters, training_loss_list, training_accuracy_list, test_loss_list, test_accuracy_list

def get_positive_integer(prompt):
    while True:
        try:
            value = int(input(prompt))
            if value > 0:
                return value
            else:
                print("Invalid input. Please enter an integer greater than zero.")
        except ValueError:
            print("Invalid input. Please enter an integer greater than zero.")

def get_positive_float(prompt):
    while True:
        try:
            value = float(input(prompt))
            if value > 0:
                return value
            else:
                print("Invalid input. Please enter a float greater than zero.")
        except ValueError:
            print("Invalid input. Please enter a float greater than zero.")

def hyperparameters(X_train):
    n_features = X_train.shape[0]
    n_hidden_layers = get_positive_integer("Enter the number of hidden layers (2-5): ")
    n_neurons_per_layer = []
    for i in range(n_hidden_layers):
        neurons = get_positive_integer(f"Enter the number of neurons in layer {i+1}: ")
        n_neurons_per_layer.append(neurons)

    num_iterations = get_positive_integer("Enter the number of iterations: ")
    learning_rate = get_positive_float("Enter the learning rate in 0.001 - 0.99: ")
    epoch_info = get_positive_integer("Enter how often to display epoch information: ")

    total_parameters = n_features * n_neurons_per_layer[0] + n_neurons_per_layer[0]
    for i in range(1, n_hidden_layers):
        total_parameters += n_neurons_per_layer[i-1] * n_neurons_per_layer[i] + n_neurons_per_layer[i]
    total_parameters += n_neurons_per_layer[-1] * 2 + 2

    print("Total number of parameters to estimate:", total_parameters)

    return n_features, n_hidden_layers, n_neurons_per_layer, num_iterations, learning_rate, epoch_info

def hyperparameter_tuning(X_train, y_train, X_test, y_test, num_iterations_list,
                          learning_rate_list, epoch_info, n_features,
                          n_hidden_layers_list, quiet_model):
    results = []

    # Generate all possible combinations of hyperparameters
    hyperparameter_combinations = list(itertools.product(
        num_iterations_list, learning_rate_list, n_hidden_layers_list))

    # Using tqdm for the progress bar
    for num_iterations, learning_rate, n_hidden_layers in tqdm(
            hyperparameter_combinations, desc="Hyperparameter Tuning", unit="combination"):

        # Run the model with the current combination of hyperparameters
        parameters, training_loss, training_accuracy, test_loss, test_accuracy = quiet_model(
            X_train, y_train, X_test, y_test, num_iterations, learning_rate,
            epoch_info, n_features, len(n_hidden_layers), n_hidden_layers)

        # Extract the last values of the desired metrics
        last_training_loss = training_loss[-1]
        last_training_accuracy = training_accuracy[-1]
        last_test_loss = test_loss[-1]
        last_test_accuracy = test_accuracy[-1]

        # Store the results
        results.append({
            'iterations': num_iterations,
            'learning_rate': learning_rate,
            'hidden_layers': n_hidden_layers,
            'train_acc': last_training_accuracy,
            'test_accu': last_test_accuracy,
            'train_loss': last_training_loss,
            'test_loss': last_test_loss
        })

    # Convert the list of dictionaries to a DataFrame
    results_df = pd.DataFrame(results)

    # Sorting the results by test_loss in ascending order
    results_df = results_df.sort_values(by='test_loss', ascending=True)
    print(results_df)

    return results_df

"""#Model plots"""

def plot_loss(training_loss_list, test_loss_list):
    """Plot training and test loss over iterations and show final values in the title."""
    plt.figure(figsize=(6, 4))
    plt.plot(training_loss_list, label='Training Loss')
    plt.plot(test_loss_list, label='Test Loss', linestyle='--')
    plt.xlabel('Iterations')
    plt.ylabel('Loss')

    # Get the final values of training and test loss
    final_train_loss = training_loss_list[-1]
    final_test_loss = test_loss_list[-1]

    # Update the title to include the final values, formatted in three lines and blue
    title_text = (f'Training vs Test Loss\n'
                  f'Final Train Loss={final_train_loss:.4f}\n'
                  f'Final Test Loss={final_test_loss:.4f}')

    # Set the title with the specified color and font size
    plt.title(title_text, fontsize=12, color='blue')
    plt.legend()
    plt.show()

def plot_accuracy(training_accuracy_list, test_accuracy_list):
    """Plot training and test accuracy over iterations and show final values in the title."""
    plt.figure(figsize=(6, 4))
    plt.plot(training_accuracy_list, label='Training Accuracy')
    plt.plot(test_accuracy_list, label='Test Accuracy', linestyle='--')
    plt.xlabel('Iterations')
    plt.ylabel('Accuracy')

    # Get the final values of training and test accuracy
    final_train_accuracy = training_accuracy_list[-1]
    final_test_accuracy = test_accuracy_list[-1]

    # Update the title to include the final values, formatted in three lines and blue
    title_text = (f'Training vs Test Accuracy\n'
                  f'Final Train Accuracy={final_train_accuracy:.4f}\n'
                  f'Final Test Accuracy={final_test_accuracy:.4f}')

    # Set the title with the specified color and font size
    plt.title(title_text, fontsize=12, color='blue')
    plt.legend()
    plt.show()